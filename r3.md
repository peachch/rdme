We sincerely appreciate your thorough and detailed feedback, which undoubtedly greatly assists us in improving our paper. 
Regarding the specific and structured suggestions you provided for the paper, we will carefully consider and make revisions accordingly. 
At this stage, we would like to address your concerns as follows.


1\. "What do you even mean by ground truth in this context? 

Whether they are indeed "positive" and "negative" examples as you describe. Why not just use the rationales verbatim?"   
    - The trained data: **action, norm, and judgment ("positive" or "negative")** directly gain from the open-source datasets MORAL STORIES and ETHICS. 
    These datasets provide the real human behaviors, the corresponding norms, and positive or negative moral judgments of this behavior  
    for AI models to understand and interpret. 
    - Why don't we only use rationales as the final results or training data? 
    Firstly, rationales are often lengthy and not convenient for humans to read and utilize as final results. 
    Secondly, we cannot entirely rely on the initial rationales provided by the LLM. 
    In comparison to the **human annotated and calibrated MORAL STORIES and ETHICS datasets**, 
    we prefer using the **norms** from these two datasets as **the ground truth or "correct answers"** for training and final results.
    
2\. "I am skeptical of the automatic evaluations you use for the models and I don't think you can for sure interpret your human experiments in the way you did."  

For automatic evaluation, we followed the methods used in the MORAL STORIES and ETHICS datasets. 
Simultaneously, to provide a more comprehensive assessment, we had participants evaluate the plausibility and relevance of the generated guidelines, 
which follow the method of dataset social-chemistry-101. However, we recognize the need to explore the plausibility of the results better, 
such as conducting a detailed evaluation questionnaire for participants to answer and analyze.

3\. "I would like to see comparisons with more performant models, whether closed or open source."   
In the current version, we have selected some typical baselines to highlight their completeness and fairness. As for more baselines, they will be added to the appendix.    
Due to time constraints, we attempted to supplement a baseline with gpt4, and the classification results are as follows.

|       | MORAL STORIES | ETHICS     |
| :---        |    :----:   |          :---: |
| Accurancy      |   0.73     | 0.76   |
| F1   |   0.73      |   0.75    |

4\. "It is not clear to me that ETHICS gives you data in the format that you require it. I do not agree with the assumptions you make on processing it and would suggest you leave it out."   
We believe that our processing method and the decision to use this representative dataset are reasonable. Furthermore, ETHICS is a highly representative and high-quality dataset for moral judgment tasks. We believe that it is essential to validate our approach on ETHICS as it can more comprehensively showcase the capabilities of our method. There are the detailed reasons. 

The data provided by ETHICS does not fully meet our requirements. ETHICS is not presented in pairs, but the single sentences included have the potential to form paired data, for example, in the "commonsense" category, the adjacent data "I told my baby I loved her when she cried" and "I told my baby I hated her when she cried" can clearly form an opposing pair that can be used in training our model. Therefore, as stated in lines 391-403 and in Appendix A.4, we provide a detailed processing procedure. Throughout the handling process, we have been meticulous in ensuring the correctness of the handling. 

5\. "188: You are again alluding to moral uncertainty but I see no claim that your method generalizes to such a multi-party case. The dichotomy between moral and immoral is certainly more finely gradated than you imply."   
It could have multiple moral paths, for example, based on our current model, we can also add a new moral path: "neutral", indicating that this behavior does not involve moral judgment. Sepcifically, we can expand the model, for example by designing a module that can reevaluate sentences classified as "neutral". Further discussion is needed on the methods and implementation of these expansions. Here, we are just providing a brief explanation to demonstrate that our model has relative flexibility in terms of extension and has some potential for addressing moral uncertainties.

6\. "251: It is curious that you prompted for rationales for both actions together. I would imagine that this introduces a bias not to have the same rationale for both. Is that a desirable bias? Is this something you can compare your method with?"

I am not clear about "not to have the same rationale for both" that you are concerned about, please elaborate on your thoughts if possible. Let me briefly explain why we are grouping two actions together. Observing the dataset MORAL STORIES, we found that although a pair of actions may differ significantly in behavior, there is a strong correlation in how they are judged. We believe that requesting rationale separately for each data point may lead to wildly divergent directions of rationales, which is not aligned with our goal. Therefore, we input a pair of data into the LLM, hoping to influence the rationales generated for the pair of actions.

7\. "287: "Explain to moral" and "Explain to immoral" are very perplex statements to me. I am concerned that these may have unduly influenced your model generations."



