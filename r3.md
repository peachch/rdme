We sincerely appreciate your thorough and detailed feedback, which undoubtedly greatly assists us in improving our paper. 
Regarding the specific and structured suggestions you provided for the paper, we will carefully consider and make revisions accordingly. 
At this stage, we would like to address your concerns as follows.


1\. "What do you even mean by ground truth in this context? 

Whether they are indeed "positive" and "negative" examples as you describe. Why not just use the rationales verbatim?"   
    - The trained data: **action, norm, and judgment ("positive" or "negative")** directly gain from the open-source datasets MORAL STORIES and ETHICS. 
    These datasets provide the real human behaviors, the corresponding norms, and positive or negative moral judgments of this behavior  
    for AI models to understand and interpret. 
    - Why don't we only use rationales as the final results or training data? 
    Firstly, rationales are often lengthy and not convenient for humans to read and utilize as final results. 
    Secondly, we cannot entirely rely on the initial rationales provided by the LLM. 
    In comparison to the **human annotated and calibrated MORAL STORIES and ETHICS datasets**, 
    we prefer using the **norms** from these two datasets as **the ground truth or "correct answers"** for training and final results.
    
2\. "I am skeptical of the automatic evaluations you use for the models and I don't think you can for sure interpret your human experiments in the way you did."  

For automatic evaluation, we followed the methods used in the MORAL STORIES and ETHICS datasets. 
Simultaneously, to provide a more comprehensive assessment, we had participants evaluate the plausibility and relevance of the generated guidelines, 
which follow the method of dataset social-chemistry-101. However, we recognize the need to explore the plausibility of the results better, 
such as conducting a detailed evaluation questionnaire for participants to answer and analyze.

3\. "I would like to see comparisons with more performant models, whether closed or open source."   
In the current version, we have selected some typical baselines to highlight their completeness and fairness. As for more baselines, they will be added to the appendix.    
Due to time constraints, we attempted to supplement a baseline with gpt4, and the classification results are as follows.

|       | MORAL STORIES | ETHICS     |
| :---        |    :----:   |          :---: |
| Accurancy      |        | 0.758   |
| F1   |         |   0.750    |

4\. "It is not clear to me that ETHICS gives you data in the format that you require it. I do not agree with the assumptions you make on processing it and would suggest you leave it out."   
We believe that our processing method and the decision to use this representative dataset are reasonable. Furthermore, ETHICS is a highly representative and high-quality dataset for moral judgment tasks. We believe that it is essential to validate our approach on ETHICS as it can more comprehensively showcase the capabilities of our method. There are the detailed reasons. 

The data provided by ETHICS does not fully meet our requirements. ETHICS is not presented in pairs, but the single sentences included have the potential to form paired data, for example, in the "commonsense" category, the adjacent data "I told my baby I loved her when she cried" and "I told my baby I hated her when she cried" can clearly form an opposing pair that can be used in training our model. Therefore, as stated in lines 391-403 and in Appendix A.4, we provide a detailed processing procedure. Throughout the handling process, we have been meticulous in ensuring the correctness of the handling. 

5\. "You are again alluding to moral uncertainty but I see no claim that your method generalizes to such a multi-party case. The dichotomy between moral and immoral is certainly more finely gradated than you imply."   


